#!/usr/bin/env python

"""
This is a program for validating GeneLab pipeline processed amplicon datasets.
"""

import os
import sys
import argparse
import textwrap
import pandas as pd
import zipfile
from statistics import mean, median

parser = argparse.ArgumentParser(description = "This program validates a GeneLab pipeline processed amplicon dataset.  It is intended to \
                                             only be run after `GL-gen-processed-amplicon-readme` has been run successfully.")
required = parser.add_argument_group('required arguments')

required.add_argument("-g", "--GLDS-ID", help = 'GLDS ID (e.g. "GLDS-276")', action = "store", required = True)
required.add_argument("-r", "--runsheet", help = "A 3-column (single-end) or 4-column (paired-end) input file (sample_id, forward, [reverse,] paired) used to run the processing pipeline. \
                        This is the value set to the parameter --csv_file when run the processing pipeline with a csv file as input \
                        otherwise it is the GLfile.csv in the GeneLab directory if --GLDS_accession was used as input.", action = "store", required = True)
parser.add_argument("--output", 
                    help = 'Name of output log file (default: "<GLDS-ID>_<output_prefix>amplicon-validation.log", with appended prefix if one is provided)',
                    default = "", action = "store")
parser.add_argument("-p", "--output-prefix", help = "Output additional file prefix if there is one", action = "store", default = "")
parser.add_argument("-l", "--V_V_guidelines_link", help = "Validation and verification guidelines link", action = "store", 
                    default = "https://genelab-tools.arc.nasa.gov/confluence/pages/viewpage.action?pageId=2428598")
parser.add_argument("--zip_targets", help = "A comma separated list of target files and/or directories to check in processing_info.zip",
                     action = "store", default = "Snakefile,config.yaml,envs/,logs/,scripts/,unique-sample-IDs.txt")
parser.add_argument("--assay_suffix", help = "Genelab assay suffix", action = "store", default = "_GLAmpSeq")
parser.add_argument("--raw_suffix", help = "Raw reads suffix", action = "store", default ="_raw.fastq.gz")
parser.add_argument("--raw_R1_suffix", help = "Raw forward reads suffix", action = "store", default = "_R1_raw.fastq.gz")
parser.add_argument("--raw_R2_suffix", help = "Raw reverse reads suffix", action = "store", default = "_R2_raw.fastq.gz")
parser.add_argument("--primer_trimmed_suffix", help = "Trimmed reads suffix", action = "store", default = "_trimmed.fastq.gz")
parser.add_argument("--primer_trimmed_R1_suffix", help = "Trimmed forward reads suffix", action = "store", default = "_R1_trimmed.fastq.gz")
parser.add_argument("--primer_trimmed_R2_suffix", help = "Trimmed reverse reads suffix", action = "store", default = "_R2_trimmed.fastq.gz")
parser.add_argument("--filtered_suffix", help = "Filtered reads suffix", action = "store", default = "_filtered.fastq.gz")
parser.add_argument("--filtered_R1_suffix", help = "Filtered forward reads suffix", action = "store", default = "_R1_filtered.fastq.gz")
parser.add_argument("--filtered_R2_suffix", help = "Filtered reverse reads suffix", action = "store", default = "_R2_filtered.fastq.gz")
parser.add_argument("--processing_zip_file", help = "Specifies the location of processing_info.zip", 
                    action = "store", default = "processing_info.zip")
parser.add_argument("--readme", help = "Specifies the location of README.txt", 
                    action = "store", default = "README.txt")
parser.add_argument("--raw_reads_dir", help = "Specifies the location of the raw reads directory if they are to be included", action = "store", default = "")
parser.add_argument("--fastqc_dir", help = "Specifies the location of fastqc and multiqc reports directory", 
                    action = "store", default = "FastQC_Outputs/")
parser.add_argument("--filtered_reads_dir", help = "Specifies the location of the filtered reads directory", 
                    action = "store", default = "Filtered_Sequence_Data/")
parser.add_argument("--trimmed_reads_dir", help = "Specifies the location of the trimmed reads directory", 
                    action = "store", default = "Trimmed_Sequence_Data/")
parser.add_argument("--final_outputs_dir", help = "Specifies the location of the final outputs directory.", 
                    action = "store", default = "Final_Outputs/")
parser.add_argument("--single-ended", help = "Add this flag if data are single-end sequencing.", action = "store_true")
parser.add_argument("--primers-already-trimmed", help = "Add this flag if primers were trimmed prior to GeneLab processing, \
                    therefore there are no trimmed sequence data", action = "store_true")
parser.add_argument("--R1-used-as-single-ended-data", help = "Provide this flag if processing only R1 reads as single-end (as the expected raw \
                    filename suffixes will have 'R1' in there)", 
                    action = "store_true")

if len(sys.argv)==1:
    parser.print_help(sys.stderr)
    sys.exit(0)

args = parser.parse_args()


################################################################################

# Setting some colors
tty_colors = {
    'green' : '\033[0;32m%s\033[0m',
    'yellow' : '\033[0;33m%s\033[0m',
    'red' : '\033[0;31m%s\033[0m'
}


######################### Aesthetic functions ################################
def color_text(text, color='green'):
    if sys.stdout.isatty():
        return tty_colors[color] % text
    else:
        return text


def wprint(text):
    """ Print wrapper """

    print(textwrap.fill(text, width = 80, initial_indent="  ", 
          subsequent_indent="  ", break_on_hyphens=False))
    

def report_failure(validation_log,  message, color = "yellow", write_log = True):
    print("")
    wprint(color_text(message, color))
    print("\nValidation failed.\n")

    if write_log:
        with open(validation_log, "a") as log:
            log.write(message + "\n" + "Validation failed." + "\n\n")
    sys.exit(1)

######################### End of Aesthetic functions ################################


############ Main functions ##############

def setup_log(validation_log, V_V_guidelines_link):
    """ Writes validation log's header """

    with open(validation_log, "w") as log:
        log.write(f"Performing baseline Amplicon V+V as per: {V_V_guidelines_link}\n\n")
        command_run = " ".join(sys.argv)
        log.write(f"Validation program executed as:\n    {command_run}\n\n")


def append_message_to_log(validation_log, message, one_return = False):
    """ Appends line to validation log with one or two newline characters """

    with open(validation_log, "a") as log:
        if one_return:
            log.write(f"{message}\n")
        else:
            log.write(f"{message}\n\n")


def check_for_file_and_contents(validation_log, file_path):
    """ Used by various functions to check if a file exists and that it is not empty"""

    if not os.path.exists(file_path):
        report_failure(validation_log, "The expected file '" + str(file_path) + "' does not exist.")
    if not os.path.getsize(file_path) > 0:
        report_failure(validation_log, "The file '" + str(file_path) + "' is empty.")

def check_expected_directories(validation_log, expected_dirs):
    """ Checks that the expected directories exist """

    for directory in expected_dirs:
        if not os.path.isdir(directory):
            report_failure(validation_log, "The directory '" + str(directory) + "' was expected but not found.")

def read_samples(file_path):
    """ Getting sample names from runsheet and reading them into list """

    sample_names = []

    with open(file_path) as f:
        next(f)  # skip header line
        for line in f:
            sample_names.append(line.strip().split(",")[0])

    return sample_names

def check_multiqc_outputs(validation_log, sample_names, multiqc_zip, 
                          multiqc_stats_file_path, R1_suffix,
                          R2_suffix, unpaired_suffix, prefix, isSingle_ended):
    """ Makes sure all samples' read files are in the multiqc outputs """

    # Checking raw
    zip_file = zipfile.ZipFile(multiqc_zip)

    df = pd.read_csv(zip_file.open(multiqc_stats_file_path), sep = "\t", usecols = ["Sample"])

    file_prefixes_in_multiqc = df["Sample"].tolist()
    
    # If paired-end
    if not isSingle_ended:

        R1_suffix = R1_suffix.split(".")[0].replace(f"_{prefix}", "")
        R2_suffix = R2_suffix.split(".")[0].replace(f"_{prefix}", "")

        for sample in sample_names:
            if not sample + R1_suffix in file_prefixes_in_multiqc:
                report_failure(validation_log, f"The {prefix} multiqc output is missing the expected '" + \
                               sample + R1_suffix + "' entry.")
            if not sample + R2_suffix in file_prefixes_in_multiqc:
                report_failure(validation_log, f"The {prefix} multiqc output is missing the expected '" + \
                               sample + R2_suffix + "' entry.")
    # If single-end
    else:

        suffix = unpaired_suffix.split(".")[0].replace(f"_{prefix}", "")

        for sample in sample_names:
            if not sample + suffix in file_prefixes_in_multiqc and not sample in file_prefixes_in_multiqc:
                report_failure(validation_log, f"The {prefix} multiqc output is missing the expected '" + \
                               sample + suffix + "' entry.")

                
def check_fastq_files(validation_log, sample_names, reads_dir, 
                      unpaired_suffix, R1_suffix, R2_suffix, isSingle_ended):
    """ Makes sure all expected read fastq files exist and hold something """

    for sample in sample_names:
        ## If paired-end
        if not isSingle_ended:
            check_for_file_and_contents(validation_log, os.path.join(reads_dir, sample + R1_suffix))
            check_for_file_and_contents(validation_log, os.path.join(reads_dir, sample + R2_suffix))
        ## If single-end
        else:
            check_for_file_and_contents(validation_log, os.path.join(reads_dir, sample + unpaired_suffix))


def get_files_in_dir(dir_path):

    return([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])


def check_amplicon_intermediate_log_files(validation_log, reads_dir, expected_outputs_or_suffixes):

    output_files_present = get_files_in_dir(reads_dir)

    for entry in expected_outputs_or_suffixes:

        if not any(output_file.endswith(entry) for output_file in output_files_present):
            report_failure(validation_log, "An output file named or ending with '" + str(entry) + \
                           "' was expected but not found in " + str(reads_dir) + ".")

def check_general_fasta_format(validation_log, file_path):
    """ 
       Check that a fasta file is formatted properly i.e. 
       the number of headers equals the number of sequences
    """
    if not os.path.getsize(file_path) > 0:
        report_failure(validation_log, "The fasta file '" + str(file_path) + \
                       "' is empty but isn't expected to be.")

    line_num = 0
    num_headers = 0
    num_seqs = 0

    with open(file_path) as in_file:

        for line in in_file:

            # Keeping track of current line for reporting any problems
            line_num += 1

            if line.strip().startswith(">"):
                num_headers += 1
            else:
                num_seqs += 1

            if num_headers != num_seqs + 1 and num_headers != num_seqs:
                report_failure(validation_log, "Fasta file '" + str(file_path) + \
                               "' does not seem to be formatted properly. Problem detected at line " + \
                                str(line_num) + ".")


def check_amplicon_final_outputs(validation_log, final_outputs_dir, expected_final_outputs_or_suffixes):
    """ Makes sure outputs exist and checks formatting """

    # Getting list of files in output dir
    output_files_present = get_files_in_dir(final_outputs_dir)

    # Making sure none of them is absent and if present it is not empty
    for output_file in output_files_present:
        check_for_file_and_contents(validation_log, os.path.join(final_outputs_dir, output_file))

    # Checking all desired output types exist
    for entry in expected_final_outputs_or_suffixes:

        if not any(output_file.endswith(entry) for output_file in output_files_present):
            report_failure(validation_log, "An output file named or ending with '" + str(entry) + \
                           "' was expected but not found in " + str(final_outputs_dir) + ".")

    # Checking general fasta format is met
    fasta_files_in_output_dir = [output_file for output_file in output_files_present if output_file.endswith(".fasta")]

    for fasta_file in fasta_files_in_output_dir:
        check_general_fasta_format(validation_log, os.path.join(final_outputs_dir, fasta_file))


def check_amplicon_processing_zip(validation_log, processing_zip_file, expected_zip_contents):
    """ This just makes sure a processing zip exists along with its expected content """

    check_for_file_and_contents(validation_log, processing_zip_file)

    with zipfile.ZipFile(processing_zip_file) as zip_obj:

        entries = zip_obj.namelist()

    ROOT_DIR = entries[0]

    for item in expected_zip_contents:
        if ROOT_DIR + item not in entries:
            report_failure(validation_log, "The '" + str(processing_zip_file) + \
                           "' does not have '" + str(item) + "' as expected.")


def report_success(validation_log):
    print("")
    wprint(color_text("Validation has completed successfully :)", "green"))
    print(f"\n  Log written to: '{validation_log}'\n")

    with open(validation_log, "a") as log:

        log.write("   -----------------------------------------------------------------------------\n")
        log.write("                         Validation completed successfully." + "\n")
        log.write("   -----------------------------------------------------------------------------\n")

def gen_stats(list_of_ints):

    """ Returns min, max, mean, median of input integer list """

    min_val = min(list_of_ints)
    max_val = max(list_of_ints)

    mean_val = round(mean(list_of_ints), 2)
    median_val = int(median(list_of_ints))
    
    return(min_val, max_val, mean_val, median_val)


def get_read_count_stats(validation_log, prefix, multiqc_zip, multiqc_stats_file_path):
    
    """ Grabs read counts and summarizes """

    zip_file = zipfile.ZipFile(multiqc_zip)
    #read_count_column = 6
    #df = pd.read_csv(zip_file.open(multiqc_stats_file_path), sep = "\t", usecols = [read_count_column])
    df = pd.read_csv(zip_file.open(multiqc_stats_file_path), sep = "\t")
    df = df.iloc[:,[-1]] # retrieve the last column which is reads counts column

    # Test to see if the sequence counts are presented as raw counts 
    # or as decimal representation of a million 
    max_count = df.iloc[:,0].max()
    if not isinstance(max_count, int):
        df = df * 1000000  # convert to raw counts

    df.columns = ["counts"]
    counts = df.counts.tolist()

    # Getting rid of decimals
    counts = [ int(round(i, 0)) for i in counts ]

    Min, Max, Mean, Median = gen_stats(counts)


    print(f"\n  {prefix.title()} read count summary:")
    print("    {:<10} {:>0}".format("Min:", Min))
    print("    {:<10} {:>0}".format("Max:", Max))
    print("    {:<10} {:>0}".format("Mean:", Mean))
    print("    {:<10} {:>0}".format("Median:", Median))

    with open(validation_log, "a") as log:

        log.write(f"\n  {prefix.title()} read count summary:")
        log.write("\n    {:<10} {:>0}".format("Min:", Min))
        log.write("\n    {:<10} {:>0}".format("Max:", Max))
        log.write("\n    {:<10} {:>0}".format("Mean:", Mean))
        log.write("\n    {:<10} {:>0}".format("Median:", Median))


def main():

    ### Variable setup ###
    output_prefix = str(args.output_prefix)
    fastqc_dir = str(args.fastqc_dir)
    filtered_reads_dir = str(args.filtered_reads_dir)
    processing_zip_file = str(args.processing_zip_file)

    # Just in case user only specified --R1-used-as-single-ended, but didn't specify --single-ended
    if args.R1_used_as_single_ended_data:
        args.single_ended = True

    V_V_guidelines_link = str(args.V_V_guidelines_link)

    assay_suffix = str(args.assay_suffix)

    trimmed_reads_dir = str(args.trimmed_reads_dir)
    final_outputs_dir = str(args.final_outputs_dir)

    raw_multiqc_zip = str(output_prefix) + f"raw_multiqc{assay_suffix}_report.zip"
    filtered_multiqc_zip = str(output_prefix) + f"filtered_multiqc{assay_suffix}_report.zip"
    raw_multiqc_stats_file_path = "raw_multiqc_report/raw_multiqc_data/multiqc_general_stats.txt"
    filtered_multiqc_stats_file_path = "filtered_multiqc_report/filtered_multiqc_data/multiqc_general_stats.txt"

    raw_suffix = str(args.raw_suffix)
    raw_R1_suffix = str(args.raw_R1_suffix)
    raw_R2_suffix = str(args.raw_R2_suffix)

    primer_trimmed_suffix = str(args.primer_trimmed_suffix)
    primer_trimmed_R1_suffix = str(args.primer_trimmed_R1_suffix)
    primer_trimmed_R2_suffix = str(args.primer_trimmed_R2_suffix)
    filtered_suffix = str(args.filtered_suffix)
    filtered_R1_suffix = str(args.filtered_R1_suffix)
    filtered_R2_suffix = str(args.filtered_R2_suffix)

    expected_trimmed_outputs_or_suffixes = [output_prefix + f"cutadapt{assay_suffix}.log",
                                             output_prefix + f"trimmed-read-counts{assay_suffix}.tsv"]
    expected_filtered_outputs_or_suffixes = [f"filtered-read-counts{assay_suffix}.tsv"]
    expected_final_outputs_or_suffixes = [".fasta", output_prefix + f"counts{assay_suffix}.tsv",
                                           output_prefix + f"taxonomy{assay_suffix}.tsv",
                                           ".biom.zip", 
                                           output_prefix + f"taxonomy-and-counts{assay_suffix}.tsv",
                                           output_prefix + f"read-count-tracking{assay_suffix}.tsv"]

    expected_dirs = [fastqc_dir, filtered_reads_dir, final_outputs_dir]

    if args.raw_reads_dir != "":

        expected_dirs.append(args.raw_reads_dir)

    if not args.primers_already_trimmed:

        expected_dirs.append(trimmed_reads_dir)

    if args.output == "":
        validation_log = str(args.GLDS_ID) + "_" + output_prefix + "amplicon-validation.log"
    else:
        validation_log = str(args.output)


    # Initializing the log file
    setup_log(validation_log, V_V_guidelines_link)
    append_message_to_log(validation_log, f"Summary of checks:")

    # Check if README.txt exists
    check_for_file_and_contents(validation_log, args.readme)
    append_message_to_log(validation_log, f"    - populated {args.readme} detected")
    # Check if the expected directories exist
    check_expected_directories(validation_log, expected_dirs)
    # Retrieve unique sample names from the sample IDs file
    sample_names = read_samples(args.runsheet)

    # Check raw multiqc outputs
    raw_multiqc_zip = os.path.join(fastqc_dir, raw_multiqc_zip)
    raw_prefix = "raw"
    check_multiqc_outputs(validation_log, sample_names, raw_multiqc_zip, 
                          raw_multiqc_stats_file_path, raw_R1_suffix,
                          raw_R2_suffix, raw_suffix, raw_prefix, args.single_ended)
    append_message_to_log(validation_log, f"    - all expected samples were found in raw multiqc files in {fastqc_dir}")

    # Check filtered multiqc outputs
    filtered_multiqc_zip = os.path.join(fastqc_dir, filtered_multiqc_zip)
    filtered_prefix = "filtered"   
    check_multiqc_outputs(validation_log, sample_names, filtered_multiqc_zip,
                          filtered_multiqc_stats_file_path, filtered_R1_suffix, 
                          filtered_R2_suffix, filtered_suffix, filtered_prefix, args.single_ended)
    append_message_to_log(validation_log, f"    - all expected samples were found in filtered multiqc files in {fastqc_dir}")

    # Raw reads
    if args.raw_reads_dir != "":
        check_fastq_files(validation_log, sample_names, args.raw_reads_dir, 
                          raw_suffix, raw_R1_suffix, raw_R2_suffix, args.single_ended)
        append_message_to_log(validation_log, f"    - all expected fastq read files were found in {args.raw_reads_dir}")

    # Trimmed reads
    if not args.primers_already_trimmed:
        check_fastq_files(validation_log, sample_names, trimmed_reads_dir, 
                          primer_trimmed_suffix, primer_trimmed_R1_suffix,
                          primer_trimmed_R2_suffix, args.single_ended)
        append_message_to_log(validation_log, f"    - all expected fastq read files were found in {trimmed_reads_dir}")
        check_amplicon_intermediate_log_files(validation_log, trimmed_reads_dir,
                                               expected_trimmed_outputs_or_suffixes)
        for file in expected_trimmed_outputs_or_suffixes:
            append_message_to_log(validation_log, f"    - {file} was found in {trimmed_reads_dir}")
        append_message_to_log(validation_log, f"    - all expected files were found in {trimmed_reads_dir}")

    # Filtered reads
    check_fastq_files(validation_log, sample_names, filtered_reads_dir, 
                      filtered_suffix, filtered_R1_suffix, 
                      filtered_R2_suffix, args.single_ended)
    append_message_to_log(validation_log, f"    - all expected fastq read files were found in {filtered_reads_dir}")
    check_amplicon_intermediate_log_files(validation_log, filtered_reads_dir,
                                           expected_filtered_outputs_or_suffixes)
    for file in expected_filtered_outputs_or_suffixes:
        append_message_to_log(validation_log, f"    - {file} was found in {filtered_reads_dir}")
    append_message_to_log(validation_log, f"    - all expected files were found in the {filtered_reads_dir} directory")

    # Check that all expected outputs exists in the final outputs directory and that the fasta files are rightly formatted 
    # i.e., the number of sequence headers equals the number of sequences in the fasta file.
    check_amplicon_final_outputs(validation_log, final_outputs_dir, expected_final_outputs_or_suffixes)
    for file in expected_final_outputs_or_suffixes:
        if file.startswith("."):
            append_message_to_log(validation_log, f"    - *{file} was found in {final_outputs_dir}")
        else: 
            append_message_to_log(validation_log, f"    - {file} was found in {final_outputs_dir}") 
    append_message_to_log(validation_log, f"    - all expected files were found in the {final_outputs_dir} directory")

     # Checking for processing_info.zip
    check_for_file_and_contents(validation_log, processing_zip_file)
    append_message_to_log(validation_log, f"    - populated {processing_zip_file} detected")

    # Check that the procesing_info.zip file contains the target sequence 
    expected_zip_contents = str(args.zip_targets).split(",")
    check_amplicon_processing_zip(validation_log, processing_zip_file, expected_zip_contents)
    for file in expected_zip_contents:
        append_message_to_log(validation_log, f"    - {file} was found in {processing_zip_file}")

    report_success(validation_log)

    # Raw
    get_read_count_stats(validation_log, raw_prefix, raw_multiqc_zip, raw_multiqc_stats_file_path)
    # Filtered
    get_read_count_stats(validation_log, filtered_prefix, filtered_multiqc_zip, filtered_multiqc_stats_file_path)

if __name__ == "__main__":
    main()
